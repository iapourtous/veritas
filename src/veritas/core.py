#!/usr/bin/env python3
import os
import json
from typing import Dict, List, Any
import time
from crewai import Crew, Process
from lib.pdf_parser import PdfParser
from lib.bm25 import BM25Ranker
from lib.levenshtein import align_response, build_factual_response
from lib.agents import VeritasCrewBuilder
from lib import config

class Veritas:
    """
    Veritas - Une cha√Æne d'agents IA qui r√©pond aux questions sur un PDF sans halluciner,
    en utilisant uniquement des phrases extraites du document.
    """
    
    def __init__(self, pdf_path: str):
        """
        Initialise Veritas avec un chemin de fichier PDF
        
        Args:
            pdf_path: Chemin vers le fichier PDF √† analyser
        """
        self.pdf_path = pdf_path
        self.pdf_parser = PdfParser()
        self.bm25_ranker = BM25Ranker()
        self.all_sentences = []
        self.sentences_by_page = {}
        
        # Extraire le texte et les phrases du PDF
        print("üìÑ Extraction du texte et d√©coupage en phrases...")
        self.all_sentences, self.sentences_by_page = self.pdf_parser.extract_all_sentences(pdf_path)
        print(f"‚úÖ {len(self.all_sentences)} phrases extraites de {len(self.sentences_by_page)} pages.")
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        R√©pond √† une question en utilisant seulement les phrases du document
        
        Args:
            question: La question √† r√©pondre
            
        Returns:
            Un dictionnaire contenant:
            - question: la question pos√©e
            - answer: la r√©ponse align√©e sur les phrases du document
            - raw_answer: la r√©ponse g√©n√©r√©e avant alignement
            - source_sentences: les phrases sources utilis√©es
            - alignment_details: d√©tails de l'alignement Levenshtein
        """
        print(f"\nüìù QUESTION: {question}")
        
        # Construire et ex√©cuter la crew
        crew_builder = VeritasCrewBuilder(self.pdf_path, question)
        crew = crew_builder.build()
        
        # √âtape 1: S√©lection des pages pertinentes
        print("\nüßë‚Äç‚öñÔ∏è Agent 1: S√©lection des pages pertinentes...")
        result1 = crew.kickoff()
        
        try:
            # Extraire les pages s√©lectionn√©es
            result1_str = str(result1)
            selected_pages = json.loads(result1_str)
            selected_pages_indices = selected_pages.get("selected_pages", [])
            
            if not selected_pages_indices:
                return {
                    "question": question,
                    "answer": "Aucune page pertinente n'a √©t√© trouv√©e dans le document.",
                    "raw_answer": "",
                    "source_sentences": [],
                    "alignment_details": [],
                    "source_pages": []
                }
            
            print(f"‚úÖ {len(selected_pages_indices)} pages s√©lectionn√©es: {selected_pages_indices}")
            
            # √âtape 2: Collecter toutes les phrases des pages s√©lectionn√©es
            all_sentences_from_selected_pages = []
            for page_idx in selected_pages_indices:
                if page_idx in self.sentences_by_page:
                    all_sentences_from_selected_pages.extend(self.sentences_by_page[page_idx])
            
            # √âtape 3: Filtrage des phrases pertinentes
            print("\nüö´ Agent 2: Filtrage des phrases pertinentes...")
            from lib.agents import SentenceFilterAgent, ResponseGeneratorAgent
            
            # R√©cr√©er une nouvelle crew avec toutes les t√¢ches
            sentence_filter = SentenceFilterAgent.create()
            task2 = SentenceFilterAgent.create_task(
                sentence_filter, 
                question, 
                all_sentences_from_selected_pages
            )
            
            new_crew = Crew(
                agents=[sentence_filter],
                tasks=[task2],
                verbose=True,
                process=Process.sequential
            )
            
            result2 = new_crew.kickoff()
            
            # Extraire les phrases s√©lectionn√©es avec gestion d'erreur robuste
            result2_str = str(result2)
            # Utiliser une extraction plus robuste pour √©viter les erreurs de parsing JSON
            try:
                selected_sentences_data = json.loads(result2_str)
                selected_sentences = selected_sentences_data.get("selected_sentences", [])
            except json.JSONDecodeError:
                # En cas d'erreur de parsing JSON, extraire les phrases directement du texte
                # avec une approche d'extraction de texte simple
                print(f"‚ö†Ô∏è Erreur de parsing JSON, utilisation d'une m√©thode alternative d'extraction")
                selected_sentences = []
                
                # Chercher les phrases directement dans la r√©ponse
                import re
                # Chercher du texte entre guillemets qui ressemble √† des phrases
                matches = re.findall(r'"([^"]+)"', result2_str)
                for match in matches:
                    if len(match) > 20:  # Filtrer les courts extraits qui ne sont probablement pas des phrases
                        selected_sentences.append(match)
            
            if not selected_sentences:
                return {
                    "question": question,
                    "answer": "Aucune phrase pertinente n'a √©t√© trouv√©e dans les pages s√©lectionn√©es.",
                    "raw_answer": "",
                    "source_sentences": [],
                    "alignment_details": [],
                    "source_pages": selected_pages_indices
                }
            print(f"‚úÖ {len(selected_sentences)} phrases s√©lectionn√©es.")
            
            # √âtape 4: G√©n√©ration de la r√©ponse
            print("\nüó£Ô∏è Agent 3: G√©n√©ration de la r√©ponse...")
            response_generator = ResponseGeneratorAgent.create()
            task3 = ResponseGeneratorAgent.create_task(
                response_generator, 
                question, 
                selected_sentences
            )
            
            final_crew = Crew(
                agents=[response_generator],
                tasks=[task3],
                verbose=True,
                process=Process.sequential
            )
            
            try:
                result3 = final_crew.kickoff()
                raw_answer = str(result3)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
                raw_answer = "Impossible de g√©n√©rer une r√©ponse coh√©rente √† partir des phrases s√©lectionn√©es."
            print("‚úÖ R√©ponse g√©n√©r√©e.")
            
            # √âtape 5: Alignement Levenshtein
            print("\nüìê Alignement avec Levenshtein...")
            alignment_results = align_response(
                raw_answer, 
                selected_sentences, 
                threshold=config.MIN_SIMILARITY_THRESHOLD
            )
            
            # Construire la r√©ponse factuelle
            factual_answer = build_factual_response(alignment_results)
            
            # R√©sultat
            return {
                "question": question,
                "answer": factual_answer,
                "raw_answer": raw_answer,
                "source_sentences": selected_sentences,
                "alignment_details": alignment_results,
                "source_pages": selected_pages_indices  # Ajout des pages sources
            }
        
        except Exception as e:
            print(f"‚ùå Erreur: {str(e)}")
            return {
                "question": question,
                "answer": f"Une erreur s'est produite lors du traitement: {str(e)}",
                "raw_answer": "",
                "source_sentences": [],
                "alignment_details": [],
                "source_pages": []
            }