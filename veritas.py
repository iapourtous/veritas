#!/usr/bin/env python3
import os
import argparse
import json
from typing import Dict, List, Any
import time
from crewai import Crew, Process
from lib.pdf_parser import PdfParser
from lib.bm25 import BM25Ranker
from lib.levenshtein import align_response, build_factual_response
from lib.agents import VeritasCrewBuilder
from lib import config
from tqdm import tqdm

class Veritas:
    """
    Veritas - Une cha√Æne d'agents IA qui r√©pond aux questions sur un PDF sans halluciner,
    en utilisant uniquement des phrases extraites du document.
    """
    
    def __init__(self, pdf_path: str):
        """
        Initialise Veritas avec un chemin de fichier PDF
        
        Args:
            pdf_path: Chemin vers le fichier PDF √† analyser
        """
        self.pdf_path = pdf_path
        self.pdf_parser = PdfParser()
        self.bm25_ranker = BM25Ranker()
        self.all_sentences = []
        self.sentences_by_page = {}
        
        # Extraire le texte et les phrases du PDF
        print("üìÑ Extraction du texte et d√©coupage en phrases...")
        self.all_sentences, self.sentences_by_page = self.pdf_parser.extract_all_sentences(pdf_path)
        print(f"‚úÖ {len(self.all_sentences)} phrases extraites de {len(self.sentences_by_page)} pages.")
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        R√©pond √† une question en utilisant seulement les phrases du document
        
        Args:
            question: La question √† r√©pondre
            
        Returns:
            Un dictionnaire contenant:
            - question: la question pos√©e
            - answer: la r√©ponse align√©e sur les phrases du document
            - raw_answer: la r√©ponse g√©n√©r√©e avant alignement
            - source_sentences: les phrases sources utilis√©es
            - alignment_details: d√©tails de l'alignement Levenshtein
        """
        print(f"\nüìù QUESTION: {question}")
        
        # Construire et ex√©cuter la crew
        crew_builder = VeritasCrewBuilder(self.pdf_path, question)
        crew = crew_builder.build()
        
        # √âtape 1: S√©lection des pages pertinentes
        print("\nüßë‚Äç‚öñÔ∏è Agent 1: S√©lection des pages pertinentes...")
        result1 = crew.kickoff()
        
        try:
            # Extraire les pages s√©lectionn√©es
            result1_str = str(result1)
            selected_pages = json.loads(result1_str)
            selected_pages_indices = selected_pages.get("selected_pages", [])
            
            if not selected_pages_indices:
                return {
                    "question": question,
                    "answer": "Aucune page pertinente n'a √©t√© trouv√©e dans le document.",
                    "raw_answer": "",
                    "source_sentences": [],
                    "alignment_details": [],
                    "source_pages": []
                }
            
            print(f"‚úÖ {len(selected_pages_indices)} pages s√©lectionn√©es: {selected_pages_indices}")
            
            # √âtape 2: Collecter toutes les phrases des pages s√©lectionn√©es
            all_sentences_from_selected_pages = []
            for page_idx in selected_pages_indices:
                if page_idx in self.sentences_by_page:
                    all_sentences_from_selected_pages.extend(self.sentences_by_page[page_idx])
            
            # √âtape 3: Filtrage des phrases pertinentes
            print("\nüö´ Agent 2: Filtrage des phrases pertinentes...")
            from lib.agents import SentenceFilterAgent, ResponseGeneratorAgent
            
            # R√©cr√©er une nouvelle crew avec toutes les t√¢ches
            sentence_filter = SentenceFilterAgent.create()
            task2 = SentenceFilterAgent.create_task(
                sentence_filter, 
                question, 
                all_sentences_from_selected_pages
            )
            
            new_crew = Crew(
                agents=[sentence_filter],
                tasks=[task2],
                verbose=True,
                process=Process.sequential
            )
            
            result2 = new_crew.kickoff()
            
            # Extraire les phrases s√©lectionn√©es avec gestion d'erreur robuste
            result2_str = str(result2)
            # Utiliser une extraction plus robuste pour √©viter les erreurs de parsing JSON
            try:
                selected_sentences_data = json.loads(result2_str)
                selected_sentences = selected_sentences_data.get("selected_sentences", [])
            except json.JSONDecodeError:
                # En cas d'erreur de parsing JSON, extraire les phrases directement du texte
                # avec une approche d'extraction de texte simple
                print(f"‚ö†Ô∏è Erreur de parsing JSON, utilisation d'une m√©thode alternative d'extraction")
                selected_sentences = []
                
                # Chercher les phrases directement dans la r√©ponse
                import re
                # Chercher du texte entre guillemets qui ressemble √† des phrases
                matches = re.findall(r'"([^"]+)"', result2_str)
                for match in matches:
                    if len(match) > 20:  # Filtrer les courts extraits qui ne sont probablement pas des phrases
                        selected_sentences.append(match)
            
            if not selected_sentences:
                return {
                    "question": question,
                    "answer": "Aucune phrase pertinente n'a √©t√© trouv√©e dans les pages s√©lectionn√©es.",
                    "raw_answer": "",
                    "source_sentences": [],
                    "alignment_details": [],
                    "source_pages": selected_pages_indices
                }
            print(f"‚úÖ {len(selected_sentences)} phrases s√©lectionn√©es.")
            
            # √âtape 4: G√©n√©ration de la r√©ponse
            print("\nüó£Ô∏è Agent 3: G√©n√©ration de la r√©ponse...")
            response_generator = ResponseGeneratorAgent.create()
            task3 = ResponseGeneratorAgent.create_task(
                response_generator, 
                question, 
                selected_sentences
            )
            
            final_crew = Crew(
                agents=[response_generator],
                tasks=[task3],
                verbose=True,
                process=Process.sequential
            )
            
            try:
                result3 = final_crew.kickoff()
                raw_answer = str(result3)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
                raw_answer = "Impossible de g√©n√©rer une r√©ponse coh√©rente √† partir des phrases s√©lectionn√©es."
            print("‚úÖ R√©ponse g√©n√©r√©e.")
            
            # √âtape 5: Alignement Levenshtein
            print("\nüìê Alignement avec Levenshtein...")
            alignment_results = align_response(
                raw_answer, 
                selected_sentences, 
                threshold=config.MIN_SIMILARITY_THRESHOLD
            )
            
            # Construire la r√©ponse factuelle
            factual_answer = build_factual_response(alignment_results)
            
            # R√©sultat
            return {
                "question": question,
                "answer": factual_answer,
                "raw_answer": raw_answer,
                "source_sentences": selected_sentences,
                "alignment_details": alignment_results,
                "source_pages": selected_pages_indices  # Ajout des pages sources
            }
        
        except Exception as e:
            print(f"‚ùå Erreur: {str(e)}")
            return {
                "question": question,
                "answer": f"Une erreur s'est produite lors du traitement: {str(e)}",
                "raw_answer": "",
                "source_sentences": [],
                "alignment_details": [],
                "source_pages": []
            }

def main():
    """
    Point d'entr√©e principal
    """
    # Parser les arguments
    parser = argparse.ArgumentParser(description="Veritas - R√©ponses factuelles bas√©es sur PDF")
    parser.add_argument("pdf", help="Chemin vers le fichier PDF")
    parser.add_argument("question", help="Question √† poser au document")
    parser.add_argument("--output", "-o", help="Fichier de sortie pour le rapport complet (JSON)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Mode verbeux")
    parser.add_argument("--debug", "-d", action="store_true", help="Mode debug (affiche plus d'informations)")
    parser.add_argument("--no-query-expansion", action="store_true", help="D√©sactiver l'expansion de requ√™te pour BM25")
    
    args = parser.parse_args()
    
    # V√©rifier que le fichier PDF existe
    if not os.path.exists(args.pdf):
        print(f"‚ùå Erreur: Le fichier PDF '{args.pdf}' n'existe pas.")
        return 1
    
    # Configurer le mode debug
    if args.debug:
        os.environ["DEBUG"] = "True"
        config.DEBUG = True
    
    # Configurer l'expansion de requ√™te
    if args.no_query_expansion:
        os.environ["QUERY_EXPANSION"] = "False"
        config.QUERY_EXPANSION = False
    
    # Initialiser Veritas
    start_time = time.time()
    veritas = Veritas(args.pdf)
    
    # R√©pondre √† la question
    result = veritas.answer_question(args.question)
    
    # Afficher la r√©ponse
    print("\n" + "="*80)
    print(f"QUESTION: {result['question']}")
    print("-"*80)
    print(f"R√âPONSE: {result['answer']}")
    
    # Afficher les pages sources
    if 'source_pages' in result and result['source_pages']:
        print("-"*80)
        print("SOURCES:")
        for page_num in result['source_pages']:
            print(f"- Page {page_num+1}")
    print("="*80)
    
    # Afficher les informations suppl√©mentaires en mode verbeux
    if args.verbose:
        print("\nR√âPONSE BRUTE:")
        print(result["raw_answer"])
        
        print("\nPHRASES SOURCES:")
        for i, sentence in enumerate(result["source_sentences"]):
            print(f"{i+1}. {sentence}")
            
        print("\nD√âTAILS D'ALIGNEMENT:")
        for detail in result["alignment_details"]:
            generated_preview = detail['generated'][:100] + "..." if len(detail['generated']) > 100 else detail['generated']
            source_preview = detail['source'][:100] + "..." if detail['source'] and len(detail['source']) > 100 else (detail['source'] or 'NON ALIGN√âE')
            
            print(f"- G√©n√©r√©e: {generated_preview}")
            print(f"  Source: {source_preview}")
            print(f"  Similarit√©: {detail['similarity']:.2f}")
            print(f"  Align√©e: {detail['aligned']}")
            print()
    
    # Enregistrer le rapport complet si demand√©
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            # Utiliser ensure_ascii=False pour √©viter d'√©chapper les caract√®res Unicode
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"Rapport complet enregistr√© dans {args.output}")
    
    elapsed_time = time.time() - start_time
    print(f"\n‚è±Ô∏è Temps total: {elapsed_time:.2f} secondes")
    
    return 0

if __name__ == "__main__":
    exit(main())